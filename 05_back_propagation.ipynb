{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04544f50",
   "metadata": {},
   "source": [
    "# 오차 역전파\n",
    "## 미분 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4524210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differentiation of x=4.0 is 8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def function(x):\n",
    "    return x**2\n",
    "\n",
    "def prime_function(x):\n",
    "    return x*2   # 도함수\n",
    "\n",
    "x0 = torch.FloatTensor([4])\n",
    "y0 = prime_function(x0)\n",
    "print(\"differentiation of x={} is {:.1f}\".format(x0.item(), y0.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee133852",
   "metadata": {},
   "source": [
    "## 매개변수 개수 구하기\n",
    "### <역전파 알고리즘의 탄생 배경>\n",
    "- 경사하강법(Gradient Descent)에서 \"미분\"이 매개변수를 찾는데 중요하다는 것을 깨달음.\n",
    "- 매개변수가 \"하나\"일 경우에는 단순하게 2차원 평면에 그릴 수 있었음.\n",
    "- h/w 모델의 매개변수 개수가 점점 많아지면... ex. XOR문제도 총 9개 매개변수 필요\n",
    "- 9개의 변수를 가진 손실 함수로부터 미분을 한번에 계산하는 것은 매우 어려움.\n",
    "    - --> \"역전파 알고리즘\" 탄생 !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12d49262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(70)\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \"\"\" xor network \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.linear_ih = nn.Linear(in_features=input_size, \n",
    "                                   out_features=hidden_size)\n",
    "        \n",
    "        self.linear_ho = nn.Linear(in_features=hidden_size, \n",
    "                                   out_features=output_size)\n",
    "        \n",
    "        self.activation_layer = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = self.linear_ih(x)\n",
    "        a1 = self.activation_layer(z1)\n",
    "        z2 = self.linear_ho(a1)\n",
    "        y = self.activation_layer(z2)\n",
    "        return y\n",
    "        \n",
    "net = Network(2,2,1)\n",
    "num_params = 0\n",
    "\n",
    "for p in net.parameters():\n",
    "    num_params += p.view(-1).size(0)\n",
    "\n",
    "print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a83316a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0919, -0.1992],\n",
      "        [-0.2247, -0.3332]], requires_grad=True)\n",
      "tensor([-0.0919, -0.1992, -0.2247, -0.3332], grad_fn=<ViewBackward0>)\n",
      "4\n",
      "\n",
      "Parameter containing:\n",
      "tensor([ 0.2342, -0.5175], requires_grad=True)\n",
      "tensor([ 0.2342, -0.5175], grad_fn=<ViewBackward0>)\n",
      "2\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[0.3162, 0.2359]], requires_grad=True)\n",
      "tensor([0.3162, 0.2359], grad_fn=<ViewBackward0>)\n",
      "2\n",
      "\n",
      "Parameter containing:\n",
      "tensor([-0.5528], requires_grad=True)\n",
      "tensor([-0.5528], grad_fn=<ViewBackward0>)\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in net.parameters():\n",
    "    print(p)\n",
    "    print(p.view(-1))\n",
    "    print(p.view(-1).size()[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f827bbe",
   "metadata": {},
   "source": [
    "### <역전파 알고리즘의 핵심>\n",
    "- \"신경망은 함수의 조합, 각 함수별로 나눠서 미분을 계산하자!\"\n",
    "- 1) 연쇄법칙 2) 계산 그래프\n",
    "\n",
    "#### 1) 연쇄법칙\n",
    "- \"합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fad4bd",
   "metadata": {},
   "source": [
    "## 2) 계산 그래프 \n",
    "### z함수 순방향 전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9df3b5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([16.])\n"
     ]
    }
   ],
   "source": [
    "def z_function(t):\n",
    "    return t**2\n",
    "\n",
    "def t_function(x,y):\n",
    "    return x+y\n",
    "\n",
    "def forward(x,y):\n",
    "    t = t_function(x,y)\n",
    "    z = z_function(t)\n",
    "    return z\n",
    "\n",
    "x = torch.Tensor([1])\n",
    "y = torch.Tensor([3])\n",
    "z = forward(x,y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78346575",
   "metadata": {},
   "source": [
    "### z함수 역방향 전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "101f425c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.])\n"
     ]
    }
   ],
   "source": [
    "def z_prime(t):\n",
    "    return 2*t\n",
    "\n",
    "def t_prime():\n",
    "    return 1\n",
    "\n",
    "def backward(t):\n",
    "    dx = z_prime(t) * t_prime()\n",
    "    return dx\n",
    "\n",
    "x = torch.Tensor([1])\n",
    "y = torch.Tensor([3])\n",
    "t = t_function(x,y)\n",
    "dx = backward(t)\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30739342",
   "metadata": {},
   "source": [
    "## 역전파 수행 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "357598ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requires grad: x=True y=False, z=True\n",
      "gradient function of z: <PowBackward0 object at 0x000002A50433D250>\n",
      "dx = tensor([8.])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.Tensor().requires_grad_() 해주면...\n",
    "계산 그래프상 해당 텐서와 연관된 모든 텐서는\n",
    "역전파 수행에 관련된 정보를 기록하게 됨! \n",
    "\"\"\"\n",
    "x = torch.Tensor([1]).requires_grad_()\n",
    "y = torch.Tensor([3])\n",
    "z = forward(x,y) # 두개 더한걸 제곱\n",
    "# z: tensor([16.], grad_fn=<PowBackward0>)\n",
    "\n",
    "print(\"requires grad: x={} y={}, z={}\".format(\\\n",
    "        x.requires_grad, y.requires_grad, z.requires_grad))\n",
    "print(\"gradient function of z: {}\".format(z.grad_fn))\n",
    "# <PowBackward0 object at 0x000002A50433D250>\n",
    "\n",
    "# 역전파 수행 버튼\n",
    "z.backward() # --> 미분값 계산\n",
    "print(\"dx = {}\".format(x.grad)) # tensor([8.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cd83339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\answl\\AppData\\Local\\Temp\\ipykernel_1152\\4110045842.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  z.grad\n"
     ]
    }
   ],
   "source": [
    "z.grad\n",
    "\"\"\"\n",
    "leaf tensor가 아니어서 오류 출력!! \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9f216ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.grad # 출력값 아무것도 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "740b7f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16.], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b363b60",
   "metadata": {},
   "source": [
    "### <역전파 알고리즘의 장점>\n",
    "- 한번에 계산하던 미분을 여러 구간으로 나눠서 계산함으로써 더 빠르고 효율적인 미분 계산 가능, 미분값이 모델 안에서 어떻게 흘러가는지 알 수 있다.\n",
    "- 딥러닝 모델을 아주 작은 모듈로 나눌 수 있다.\n",
    "- PyTorch같은 딥러닝 프레임워크가 역전파를 밑단(백엔드)에서 수행해줌으로써 연구자들이 더 이상 복잡한 수식의 미분을 계산하지 않게 됐다. --> 더 많은 시간을 딥러닝 모델 등 다른 부분에 투자할 수 있게 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2352cb5",
   "metadata": {},
   "source": [
    "### <PyTorch에서 모델을 학습하는 과정>\n",
    "1. 입력&타겟 데이터 전처리\n",
    "2. 모델/손실함수/옵티마이저(경사하강법 수행) 정의\n",
    "3. 에포크 회수 결정 (데이터 첨부터 끝까지 1회 학습)\n",
    "- 1) 경사 초기화\n",
    "- 2) 순방향 전파 진행\n",
    "- 3) 손실값 계산\n",
    "- 4) 역방향 전파 수행 (오차역전파)\n",
    "- 5) 옵티마이저로 매개변수 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2758236",
   "metadata": {},
   "source": [
    "# 오차역전파\n",
    "- 오차 역전파를 위한 도구가 모두 torch.optim 모듈에 있음.\n",
    "- 확률적 경사하강법 사용 (경사하강법 기반 다양한 알고리즘 존재)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a878e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t 0.6927\n",
      "1000:\t 0.2574\n",
      "2000:\t 0.0210\n",
      "3000:\t 0.0102\n",
      "4000:\t 0.0067\n",
      "5000:\t 0.0049\n",
      "6000:\t 0.0039\n",
      "7000:\t 0.0032\n",
      "8000:\t 0.0028\n",
      "9000:\t 0.0024\n",
      "10000:\t 0.0021\n",
      "tensor([False,  True,  True, False])\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim # 오차역전파\n",
    "\n",
    "# 입력과 타깃 텐서\n",
    "x = torch.Tensor([[0,0], [0,1], [1,0], [1,1]])\n",
    "t = torch.Tensor([0,1,1,0])\n",
    "\n",
    "# XOR 네트워크 정의\n",
    "net = Network(input_size=2, hidden_size=2, output_size=1)\n",
    "\n",
    "# 손실함수 정의\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "# 경사하강법 optimizer 정의: 모델의 매개변수를 전달해줘야함.\n",
    "optimizer = optim.SGD(params=net.parameters(), lr=0.5)\n",
    "# 0.5의 학습률로 넷의 매개변수들에 확률적 경사하강법 적용\n",
    "\n",
    "# 학습 회수 결정 \n",
    "STEP = 10001\n",
    "\n",
    "# 학습과정\n",
    "for step in range(STEP):\n",
    "    # 1) 경사 초기화\n",
    "    net.zero_grad()\n",
    "    \n",
    "    # 2) 순방향 전파\n",
    "    y = net(x)\n",
    "    \n",
    "    # 3) 손실값 계산\n",
    "    loss = loss_function(y.squeeze(), t)\n",
    "    \n",
    "    # 4) 역방향 전파\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5) optimizer의 경사하강법으로 매개변수 업데이트\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 1000 == 0:\n",
    "        print(\"{}:\\t {:.4f}\".format(step, loss.item()))\n",
    "        \n",
    "# 올바른 정답을 출력하는지 테스트\n",
    "pred = net(x).ge(0.5) # greater_equal\n",
    "print(pred.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a369b20",
   "metadata": {},
   "source": [
    "torch.ge(n): n보다 크거나 같으면 True, 작으면 False (= torch.greater_equal(n))\n",
    "\n",
    "torch.gt(n): n보다 크면 True, 작거나 같으면 False\n",
    "\n",
    "torch.le : 작거나 같으면 True, 크면 False\n",
    "- torch.less_eqaul 와 동일\n",
    "- torch.lt : less than\n",
    "\n",
    "torch.eq : 같으면 True, 다르면 False\n",
    "- torch.ne 와 반대\n",
    "\n",
    "torch.equal : 텐서 차원에서 같으면 True, 다르면 False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7793daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c8b5fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True],\n",
       "        [False, False]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.greater_equal(torch.tensor([[1]]), torch.tensor([[0,1], [3,4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7223ced2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True, False, False]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.greater_equal(torch.tensor([[1]]), torch.tensor([0,1, 3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "302482fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1152\\3265754313.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgreater_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "torch.greater_equal(torch.tensor([[1,2,3,4]]), torch.tensor([[0,1], [3,4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b66fdc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0,1, 3,4]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8c56565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([4])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([9])\n",
      "tensor([[1, 2, 3],\n",
      "        [3, 4, 5],\n",
      "        [5, 6, 7]])\n",
      "tensor([1, 2, 3, 3, 4, 5, 5, 6, 7])\n",
      "tensor([[1, 2, 3],\n",
      "        [3, 4, 5],\n",
      "        [5, 6, 7]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2],\n",
    "             [3,4]])\n",
    "print(a.shape)\n",
    "print(a.view(-1).shape)\n",
    "\n",
    "b = torch.tensor([[1,2,3],\n",
    "             [3,4,5],\n",
    "                 [5,6,7]])\n",
    "print(b.shape)\n",
    "print(b.view(-1).shape)\n",
    "print(b)\n",
    "print(b.view(-1))\n",
    "print(b.view(3,-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
